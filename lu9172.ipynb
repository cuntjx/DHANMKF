{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3780552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import linalg\n",
    "from scipy.linalg import eigh\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "# import wandb\n",
    "from torch.nn.parameter import Parameter\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "from torch import optim\n",
    "import gc\n",
    "import os.path as osp\n",
    "import csv\n",
    "from typing import Optional\n",
    "from torch_geometric.typing import OptTensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import get_laplacian\n",
    "from torch.autograd import Variable\n",
    "from torch_geometric.nn import GCNConv, GATConv,RGCNConv\n",
    "from torch_geometric.nn.inits import glorot, uniform\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "from torch.nn import Sequential, Linear, ReLU, Dropout\n",
    "from scipy.special import comb\n",
    "from torch import Tensor\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "import copy\n",
    "import sklearn.metrics\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch_geometric.nn.dense.linear import Linear as gLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c468a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "cudaid = \"cpu\"\n",
    "device = torch.device(cudaid)\n",
    "\n",
    "root = r\"/home/lu/data/circRNA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572992ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271, 271)\n",
      "(218, 218)\n",
      "(271, 218)\n"
     ]
    }
   ],
   "source": [
    "seq_sim_matrix = pd.read_csv(root + r\"/gene_seq_sim.csv\", index_col=0, dtype=np.float32).to_numpy()\n",
    "str_sim_matrix = pd.read_csv(root + r\"/drug_str_sim.csv\", index_col=0, dtype=np.float32).to_numpy()\n",
    "association = pd.read_csv(root + r\"/association.csv\", index_col=0).to_numpy()\n",
    "\n",
    "\n",
    "print(seq_sim_matrix.shape)\n",
    "print(str_sim_matrix.shape)\n",
    "print(association.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439cb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense2sparse(matrix: np.ndarray):\n",
    "    mat_coo = coo_matrix(matrix)\n",
    "    edge_idx = np.vstack((mat_coo.row, mat_coo.col))\n",
    "    return edge_idx, mat_coo.data\n",
    "\n",
    "def load_data():\n",
    "    cir_sim = pd.read_csv(root + r\"/gene_seq_sim.csv\", index_col=0, dtype=np.float32).to_numpy()\n",
    "    drug_sim=pd.read_csv(root + r\"/drug_str_sim.csv\", index_col=0, dtype=np.float32).to_numpy()\n",
    "    drug_cir_ass =pd.read_csv(root + r\"/association.csv\", index_col=0).to_numpy().T\n",
    "    diag = np.diag(cir_sim)\n",
    "    if np.sum(diag) != 0:\n",
    "        cir_sim = cir_sim - np.diag(diag)\n",
    "\n",
    "    # get the edge idx of positives samplese\n",
    "    rng = np.random.default_rng(10086)\n",
    "    pos_samples, edge_attr = dense2sparse(drug_cir_ass)\n",
    "    pos_samples_shuffled = rng.permutation(pos_samples, axis=1)\n",
    "\n",
    "    # get the edge index of negative samples\n",
    "    rng = np.random.default_rng(10086)\n",
    "    neg_samples = np.where(drug_cir_ass == 0)\n",
    "    neg_samples_shuffled = rng.permutation(neg_samples, axis=1)[:, :pos_samples_shuffled.shape[1]]\n",
    "    # split positive samples into training message samples, training supervision samples, test samples\n",
    "    edge_idx_dict = dict()\n",
    "    edge_idx_dict['pos_edges'] = pos_samples_shuffled\n",
    "    edge_idx_dict['neg_edges'] = neg_samples_shuffled\n",
    "\n",
    "    return drug_sim,cir_sim, edge_idx_dict,drug_cir_ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017ad337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_matrix(matrix, k=20):\n",
    "    num = matrix.shape[0]\n",
    "    knn_graph = np.zeros(matrix.shape)\n",
    "    idx_sort = np.argsort(-(matrix - np.eye(num)), axis=1)\n",
    "    for i in range(num):\n",
    "        #将第i行最大的前k个值赋值给knn_graph(确保是对称矩阵)\n",
    "        knn_graph[i, idx_sort[i, :k + 1]] = matrix[i, idx_sort[i, :k + 1]]\n",
    "        knn_graph[idx_sort[i, :k + 1], i] = matrix[idx_sort[i, :k + 1], i]\n",
    "    return knn_graph + np.eye(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d692147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GIP_kernel(Asso_RNA_Dis):\n",
    "    # the number of row\n",
    "    nc = Asso_RNA_Dis.shape[0]\n",
    "    # initate a matrix as results matrix\n",
    "    matrix = np.zeros((nc, nc))\n",
    "    # calculate the down part of GIP fmulate\n",
    "    r = getGosiR(Asso_RNA_Dis)\n",
    "    # calculate the results matrix\n",
    "    for i in range(nc):\n",
    "        for j in range(nc):\n",
    "            # calculate the up part of GIP formulate\n",
    "            temp_up = np.square(np.linalg.norm(Asso_RNA_Dis[i, :] - Asso_RNA_Dis[j, :]))\n",
    "            if r == 0:\n",
    "                matrix[i][j] = 0\n",
    "            elif i == j:\n",
    "                matrix[i][j] = 1\n",
    "            else:\n",
    "                matrix[i][j] = np.e ** (-temp_up / r)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def getGosiR(Asso_RNA_Dis):\n",
    "    # calculate the r in GOsi Kerel\n",
    "    nc = Asso_RNA_Dis.shape[0]\n",
    "    summ = 0\n",
    "    for i in range(nc):\n",
    "        x_norm = np.linalg.norm(Asso_RNA_Dis[i, :])\n",
    "        x_norm = np.square(x_norm)\n",
    "        summ = summ + x_norm\n",
    "    r = summ / nc\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_syn_sim(A, seq_sim, str_sim, mode):\n",
    "    \"\"\"\n",
    "    :param A:\n",
    "    :param seq_sim:\n",
    "    :param str_sim:\n",
    "    :param mode: 0 = GIP kernel sim\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    GIP_c_sim = GIP_kernel(A)\n",
    "    GIP_d_sim = GIP_kernel(A.T)\n",
    "\n",
    "    if mode == 0:\n",
    "        return GIP_c_sim, GIP_d_sim\n",
    "\n",
    "    syn_c = np.zeros((A.shape[0], A.shape[0]))\n",
    "    syn_d = np.zeros((A.shape[1], A.shape[1]))\n",
    "\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[0]):\n",
    "            if seq_sim[i, j] == 0:\n",
    "                syn_c[i, j] = GIP_c_sim[i, j]\n",
    "            else:\n",
    "                syn_c[i, j] = (GIP_c_sim[i, j] + seq_sim[i, j]) / 2\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        for j in range(A.shape[1]):\n",
    "            if str_sim[i, j] == 0:\n",
    "                syn_d[i, j] = GIP_d_sim[i, j]\n",
    "            else:\n",
    "                syn_d[i, j] = (GIP_d_sim[i, j] + str_sim[i, j]) / 2\n",
    "\n",
    "    return syn_c, syn_d\n",
    "\n",
    "\n",
    "def sim_thresholding(matrix: np.ndarray, threshold):\n",
    "    matrix_copy = matrix.copy()\n",
    "    matrix_copy[matrix_copy >= threshold] = 1\n",
    "    matrix_copy[matrix_copy < threshold] = 0\n",
    "    print(f\"rest links: {np.sum(np.sum(matrix_copy))}\")\n",
    "    return matrix_copy\n",
    "\n",
    "\n",
    "# ######################################################################################################################\n",
    "\n",
    "\n",
    "def get_syn_sim_circ_drug(A, seq_sim, str_sim, k1, k2):\n",
    "    disease_sim1 = str_sim\n",
    "    circRNA_sim1 = seq_sim\n",
    "\n",
    "    GIP_c_sim = GIP_kernel(A)\n",
    "    GIP_d_sim = GIP_kernel(A.T)\n",
    "    # miRNA_sim1 = GIP_m_sim\n",
    "    m1 = new_normalization(circRNA_sim1)\n",
    "    m2 = new_normalization(GIP_c_sim)\n",
    "\n",
    "    Sm_1 = KNN_kernel(circRNA_sim1, k1)\n",
    "    Sm_2 = KNN_kernel(GIP_c_sim, k1)\n",
    "    Pm = circRNA_updating(Sm_1, Sm_2, m1, m2)\n",
    "    Pm_final = (Pm + Pm.T) / 2\n",
    "\n",
    "    d1 = new_normalization(disease_sim1)\n",
    "    d2 = new_normalization(GIP_d_sim)\n",
    "\n",
    "    Sd_1 = KNN_kernel(disease_sim1, k2)\n",
    "    Sd_2 = KNN_kernel(GIP_d_sim, k2)\n",
    "    Pd = disease_updating(Sd_1, Sd_2, d1, d2)\n",
    "    Pd_final = (Pd + Pd.T) / 2\n",
    "\n",
    "    return Pm_final, Pd_final\n",
    "\n",
    "\n",
    "def new_normalization(w):\n",
    "    m = w.shape[0]\n",
    "    p = np.zeros([m, m])\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            if i == j:\n",
    "                p[i][j] = 1 / 2\n",
    "            elif np.sum(w[i, :]) - w[i, i] > 0:\n",
    "                p[i][j] = w[i, j] / (2 * (np.sum(w[i, :]) - w[i, i]))\n",
    "    return p\n",
    "\n",
    "\n",
    "def KNN_kernel(S, k):\n",
    "    n = S.shape[0]\n",
    "    S_knn = np.zeros([n, n])\n",
    "    for i in range(n):\n",
    "        sort_index = np.argsort(S[i, :])\n",
    "        for j in sort_index[n - k:n]:\n",
    "            if np.sum(S[i, sort_index[n - k:n]]) > 0:\n",
    "                S_knn[i][j] = S[i][j] / (np.sum(S[i, sort_index[n - k:n]]))\n",
    "    return S_knn\n",
    "\n",
    "\n",
    "def circRNA_updating(S1, S2, P1, P2):\n",
    "    P = (P1 + P2) / 2\n",
    "    dif = 1\n",
    "    while dif > 0.0000001:\n",
    "        P111 = np.dot(np.dot(S1, P2), S1.T)\n",
    "        P111 = new_normalization(P111)\n",
    "        P222 = np.dot(np.dot(S2, P1), S2.T)\n",
    "        P222 = new_normalization(P222)\n",
    "        P1 = P111\n",
    "        P2 = P222\n",
    "        P_New = (P1 + P2) / 2\n",
    "        dif = np.linalg.norm(P_New - P) / np.linalg.norm(P)\n",
    "        P = P_New\n",
    "    return P\n",
    "\n",
    "\n",
    "def disease_updating(S1, S2, P1, P2):\n",
    "    P = (P1 + P2) / 2\n",
    "    dif = 1\n",
    "    while dif > 0.0000001:\n",
    "        P111 = np.dot(np.dot(S1, P2), S1.T)\n",
    "        P111 = new_normalization(P111)\n",
    "        P222 = np.dot(np.dot(S2, P1), S2.T)\n",
    "        P222 = new_normalization(P222)\n",
    "        P1 = P111\n",
    "        P2 = P222\n",
    "        P_New = (P1 + P2) / 2\n",
    "        dif = np.linalg.norm(P_New - P) / np.linalg.norm(P)\n",
    "        P = P_New\n",
    "    return P\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "def skf_normalization(w):\n",
    "    row_sum = np.sum(w, axis=0)\n",
    "    p = (w / row_sum).T\n",
    "    return p\n",
    "\n",
    "\n",
    "def skf(A, seq_sim, str_sim, k1, k2):\n",
    "    disease_sim1 = str_sim\n",
    "    circRNA_sim1 = seq_sim\n",
    "\n",
    "    GIP_c_sim = GIP_kernel(A)\n",
    "    GIP_d_sim = GIP_kernel(A.T)\n",
    "    # miRNA_sim1 = GIP_m_sim\n",
    "    m1 = skf_normalization(circRNA_sim1)\n",
    "    m2 = skf_normalization(GIP_c_sim)\n",
    "\n",
    "    Sm_1 = KNN_kernel(circRNA_sim1, k1)\n",
    "    Sm_2 = KNN_kernel(GIP_c_sim, k1)\n",
    "    Pm = skf_updating(Sm_1, Sm_2, m1, m2, 0.1)\n",
    "    nei_weight1 = neighborhood_Com(Pm, k1)\n",
    "    Pm_final = Pm * nei_weight1\n",
    "\n",
    "    d1 = skf_normalization(disease_sim1)\n",
    "    d2 = skf_normalization(GIP_d_sim)\n",
    "\n",
    "    Sd_1 = KNN_kernel(disease_sim1, k2)\n",
    "    Sd_2 = KNN_kernel(GIP_d_sim, k2)\n",
    "    Pd = skf_updating(Sd_1, Sd_2, d1, d2, 0.1)\n",
    "    nei_weight2 = neighborhood_Com(Pd, k2)\n",
    "    Pd_final = Pd * nei_weight2\n",
    "\n",
    "    return Pm_final, Pd_final\n",
    "\n",
    "\n",
    "def skf_updating(S1, S2, P1, P2, alpha):\n",
    "    P = (P1 + P2) / 2\n",
    "    dif = 1\n",
    "    while dif > 0.0000001:\n",
    "        P111 = alpha * np.dot(np.dot(S1, P2), S1.T) + (1 - alpha) * P2\n",
    "        P111 = new_normalization(P111)\n",
    "        P222 = alpha * np.dot(np.dot(S2, P1), S2.T) + (1 - alpha) * P1\n",
    "        P222 = new_normalization(P222)\n",
    "        P1 = P111\n",
    "        P2 = P222\n",
    "        P_New = (P1 + P2) / 2\n",
    "        dif = np.linalg.norm(P_New - P) / np.linalg.norm(P)\n",
    "        P = P_New\n",
    "    return P\n",
    "\n",
    "\n",
    "def neighborhood_Com(sim, k):\n",
    "    weight = np.zeros(sim.shape)\n",
    "\n",
    "    for i in range(sim.shape[0]):\n",
    "        iu = sim[i, :]\n",
    "        iu_list = np.abs(np.sort(-iu))\n",
    "        iu_nearest_list_end = iu_list[k - 1]\n",
    "        for j in range(sim.shape[1]):\n",
    "            ju = sim[:, j]\n",
    "            ju_list = np.abs(np.sort(-ju))\n",
    "            ju_nearest_list_end = ju_list[k - 1]\n",
    "\n",
    "            if sim[i, j] >= iu_nearest_list_end and sim[i, j] >= ju_nearest_list_end:\n",
    "                weight[i, j] = 1\n",
    "                weight[j, i] = 1\n",
    "            elif sim[i, j] < iu_nearest_list_end and sim[i, j] < ju_nearest_list_end:\n",
    "                weight[i, j] = 0\n",
    "                weight[j, i] = 0\n",
    "            else:\n",
    "                weight[i, j] = 0.5\n",
    "                weight[j, i] = 0.5\n",
    "\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22aca3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_the_samples(A):\n",
    "    m,n = A.shape\n",
    "    pos = []\n",
    "    neg = []\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if A[i,j] ==1:\n",
    "                pos.append([i,j,1])\n",
    "            else:\n",
    "                neg.append([i,j,0])\n",
    "    n = len(pos)\n",
    "    neg_new = random.sample(neg, n)\n",
    "    tep_samples = pos + neg_new\n",
    "    samples = random.sample(tep_samples, len(tep_samples))\n",
    "    samples = random.sample(samples, len(samples))\n",
    "    samples = np.array(samples)\n",
    "    return samples\n",
    "\n",
    "def update_Adjacency_matrix (A, test_samples):\n",
    "    m = test_samples.shape[0]\n",
    "    A_tep = A.copy()\n",
    "    for i in range(m):\n",
    "        if test_samples[i,2] ==1:\n",
    "            A_tep [test_samples[i,0], test_samples[i,1]] = 0\n",
    "    return A_tep\n",
    "\n",
    "def set_digo_zero(sim, z):\n",
    "    sim_new = sim.copy()\n",
    "    n = sim.shape[0]\n",
    "    for i in range(n):\n",
    "        sim_new[i][i] = z\n",
    "    return sim_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9bc2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaluation_metrics(pred_mat, pos_edges, neg_edges):\n",
    "    pos_pred_socres = pred_mat[pos_edges[0], pos_edges[1]]\n",
    "    neg_pred_socres = pred_mat[neg_edges[0], neg_edges[1]]\n",
    "    pred_labels = np.hstack((pos_pred_socres, neg_pred_socres))\n",
    "    true_labels = np.hstack((np.ones(pos_pred_socres.shape[0]), np.zeros(neg_pred_socres.shape[0])))\n",
    "    return get_metrics_new(true_labels, pred_labels)\n",
    "\n",
    "def get_metrics_new(real_score, predict_score):\n",
    "    real_score, predict_score = real_score.flatten(), predict_score.flatten()\n",
    "    sorted_predict_score = np.array(\n",
    "        sorted(list(set(np.array(predict_score).flatten()))))\n",
    "    sorted_predict_score_num = len(sorted_predict_score)\n",
    "    thresholds = sorted_predict_score[np.int32(\n",
    "        sorted_predict_score_num*np.arange(1, 1000)/1000)]\n",
    "    thresholds = np.mat(thresholds)\n",
    "    thresholds_num = thresholds.shape[1]\n",
    "\n",
    "    predict_score_matrix = np.tile(predict_score, (thresholds_num, 1))\n",
    "    negative_index = np.where(predict_score_matrix < thresholds.T)\n",
    "    positive_index = np.where(predict_score_matrix >= thresholds.T)\n",
    "    predict_score_matrix[negative_index] = 0\n",
    "    predict_score_matrix[positive_index] = 1\n",
    "    TP = predict_score_matrix.dot(real_score.T)\n",
    "    FP = predict_score_matrix.sum(axis=1)-TP\n",
    "    FN = real_score.sum()-TP\n",
    "    TN = len(real_score.T)-TP-FP-FN\n",
    "\n",
    "    fpr = FP/(FP+TN)\n",
    "    tpr = TP/(TP+FN)\n",
    "    ROC_dot_matrix = np.mat(sorted(np.column_stack((fpr, tpr)).tolist())).T\n",
    "    ROC_dot_matrix.T[0] = [0, 0]\n",
    "    ROC_dot_matrix = np.c_[ROC_dot_matrix, [1, 1]]\n",
    "\n",
    "    # np.savetxt(roc_path.format(i), ROC_dot_matrix)\n",
    "\n",
    "    x_ROC = ROC_dot_matrix[0].T\n",
    "    y_ROC = ROC_dot_matrix[1].T\n",
    "    auc = 0.5*(x_ROC[1:]-x_ROC[:-1]).T*(y_ROC[:-1]+y_ROC[1:])\n",
    "\n",
    "    recall_list = tpr\n",
    "    precision_list = TP/(TP+FP)\n",
    "    PR_dot_matrix = np.mat(sorted(np.column_stack(\n",
    "        (recall_list, precision_list)).tolist())).T\n",
    "    PR_dot_matrix.T[0] = [0, 1]\n",
    "    PR_dot_matrix = np.c_[PR_dot_matrix, [1, 0]]\n",
    "\n",
    "    # np.savetxt(pr_path.format(i), PR_dot_matrix)\n",
    "\n",
    "    x_PR = PR_dot_matrix[0].T\n",
    "    y_PR = PR_dot_matrix[1].T\n",
    "    aupr = 0.5*(x_PR[1:]-x_PR[:-1]).T*(y_PR[:-1]+y_PR[1:])\n",
    "\n",
    "    f1_score_list = 2*TP/(len(real_score.T)+TP-TN)\n",
    "    accuracy_list = (TP+TN)/len(real_score.T)\n",
    "    specificity_list = TN/(TN+FP)\n",
    "    # plt.plot(x_ROC, y_ROC)\n",
    "    # plt.plot(x_PR,y_PR)\n",
    "    # plt.show()\n",
    "    max_index = np.argmax(f1_score_list)\n",
    "    f1_score = f1_score_list[max_index]\n",
    "    accuracy = accuracy_list[max_index]\n",
    "    specificity = specificity_list[max_index]\n",
    "    recall = recall_list[max_index]\n",
    "    precision = precision_list[max_index]\n",
    "    print( ' auc:{:.4f} ,aupr:{:.4f},f1_score:{:.4f}, accuracy:{:.4f}, recall:{:.4f}, specificity:{:.4f}, precision:{:.4f}'.format( auc[0, 0],aupr[0, 0], f1_score, accuracy, recall, specificity, precision))\n",
    "    return [auc[0, 0], aupr[0, 0], f1_score, accuracy, recall, specificity, precision]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d67f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_edge_index_f(matrix, threshold):\n",
    "    edge_index = [[], []]\n",
    "    edge_type = []    \n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if matrix[i, j] >= 0.001:                \n",
    "                edge_index[0].append(i)\n",
    "                edge_index[1].append(j)\n",
    "                if (i < threshold) and (j < threshold):                \n",
    "                    edge_type.append(0)\n",
    "                elif (i >= threshold) and (j < threshold):                \n",
    "                    edge_type.append(1)\n",
    "                elif (i < threshold) and (j >= threshold):                \n",
    "                    edge_type.append(2)\n",
    "                else:\n",
    "                    edge_type.append(3)\n",
    "    return torch.LongTensor(edge_index), torch.LongTensor(edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead36471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_edge_index(matrix, new_A, threshold):\n",
    "    edge_index = [[], []]\n",
    "    edge_type = []\n",
    "    threshold_list = new_A.sum(1)\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if matrix[i, j] >= 0.001:                \n",
    "                edge_index[0].append(i)\n",
    "                edge_index[1].append(j)\n",
    "                if (threshold_list[i] <= threshold) and (threshold_list[j] <= threshold):                \n",
    "                    edge_type.append(0)\n",
    "                elif (threshold_list[i] <= threshold) and (threshold_list[j] > threshold):                \n",
    "                    edge_type.append(1)\n",
    "                elif (threshold_list[i] > threshold) and (threshold_list[j] <= threshold):                \n",
    "                    edge_type.append(2)\n",
    "                else:\n",
    "                    edge_type.append(3)\n",
    "    return torch.LongTensor(edge_index), torch.LongTensor(edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41dce698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_index_h(matrix, threshold_m, threshold_d):\n",
    "    edge_index = [[], []]\n",
    "    edge_type = []\n",
    "    threshold_list_m = matrix.sum(1)\n",
    "    threshold_list_d = matrix.sum(0)\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if matrix[i, j] != 0:                \n",
    "                edge_index[0].append(i)\n",
    "                edge_index[1].append(j)\n",
    "                if (threshold_list_m[i] <= threshold_m) and (threshold_list_d[j] <= threshold_d):                \n",
    "                    edge_type.append(0)\n",
    "                elif (threshold_list_m[i] <= threshold_m) and (threshold_list_d[j] > threshold_d):                \n",
    "                    edge_type.append(1)\n",
    "                elif (threshold_list_m[i] > threshold_m) and (threshold_list_d[j] <= threshold_d):                \n",
    "                    edge_type.append(2)\n",
    "                else:\n",
    "                    edge_type.append(3)\n",
    "    return torch.LongTensor(edge_index), torch.LongTensor(edge_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ea28dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGATConv(MessagePassing):\n",
    "    def __init__(self, in_hid, out_hid, \n",
    "                 num_edge_types,negative_slope=0.2,dual=True,heads=1,mask=None,global_weight=True):\n",
    "        super(DGATConv, self).__init__(aggr='add')\n",
    "\n",
    "        self.in_hid = in_hid\n",
    "        self.out_hid = out_hid\n",
    "        self.num_edge_types = num_edge_types\n",
    "        self.negative_slope=negative_slope\n",
    "        self.dual=dual\n",
    "        self.mask=mask\n",
    "        self.global_weight=global_weight\n",
    "        \n",
    "        self.rel_wi=nn.Parameter(torch.Tensor(num_edge_types,out_hid*2,1))\n",
    "\n",
    "        self.rel_bt=nn.Parameter(torch.Tensor(out_hid*2,1))\n",
    "        self.w_wi=nn.Linear(in_hid, out_hid, bias=False)\n",
    "        self.w_bt=nn.Linear(out_hid,out_hid,bias=False)\n",
    "\n",
    "        self.w_out=nn.Linear(out_hid,out_hid,bias=False)\n",
    "        self.q_trans=nn.Parameter(torch.Tensor(out_hid,1))\n",
    "\n",
    "        self.norm=nn.LayerNorm(out_hid)\n",
    "        self.norm_list=nn.ModuleList()\n",
    "        for i in range(num_edge_types):\n",
    "            self.norm_list.append(nn.LayerNorm(out_hid))\n",
    "\n",
    "\n",
    "        self.skip = nn.Parameter(torch.ones(1))\n",
    "        self.beta_weight=nn.Parameter(torch.ones(1))\n",
    "        self.overall_beta=nn.Parameter(torch.randn(num_edge_types))\n",
    "        # self.drop=Dropout(0.2)\n",
    "\n",
    "        glorot(self.rel_wi)\n",
    "        glorot(self.rel_bt)\n",
    "        glorot(self.q_trans)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_idx, edge_type):\n",
    "\n",
    "        x=self.w_wi(x)\n",
    "        out_list=[]\n",
    "        edg_list=[]\n",
    "        overall_rel=[]\n",
    "        for i in range(self.num_edge_types):\n",
    "            mask = (edge_type == i)\n",
    "            edge_index = edge_idx[:, mask]\n",
    "            if mask.sum() !=0:\n",
    "                rs=self.w_bt(F.leaky_relu(self.norm_list[i](self.propagate(edge_index, x=x,edge_type=i)),self.negative_slope))   #Nxd\n",
    "                out_list+=[rs]\n",
    "                edg_list+=[i]\n",
    "     \n",
    "            \n",
    "        if self.dual:\n",
    "            overall_beta=F.softmax(self.overall_beta,dim=0)\n",
    "\n",
    "            rs_list=[]\n",
    "            for i in range(len(edg_list)):\n",
    "                conc=torch.cat((x,out_list[i]),dim=1)                                       #Nx2d\n",
    "                rs=torch.matmul(conc,self.rel_bt)         #Nx1\n",
    "                rs_list+=[rs]\n",
    "\n",
    "            rs=torch.stack(rs_list)                                                         #rxNx1\n",
    "            beta=F.softmax(rs,dim=0)                                                        #rxNx1\n",
    "            res=0\n",
    "            if self.mask:\n",
    "                for i in self.mask:\n",
    "                    out_list[i]=torch.zeros_like(out_list[i])\n",
    "            beta_weight=torch.sigmoid(self.beta_weight)\n",
    "            for i in range(len(edg_list)):\n",
    "                if self.global_weight:\n",
    "                    res+=out_list[i]*((1-beta_weight)*beta[i]+beta_weight*overall_beta[i])\n",
    "                else:\n",
    "                    res+=out_list[i]*beta[i]\n",
    "        else:\n",
    "            res=0\n",
    "            for i in range(len(edg_list)):\n",
    "                res+=out_list[i]\n",
    "        \n",
    "        final_weight=torch.sigmoid(self.skip)\n",
    "        res = self.norm(F.gelu(res) * (final_weight) + x* (1 - final_weight))\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def message(self,edge_index,x_i, x_j,edge_type):\n",
    "        \n",
    "        node_f = torch.cat((x_i, x_j), 1)                                       #nx2d\n",
    "\n",
    "        temp = torch.matmul(node_f, self.rel_wi[edge_type]).to(x_i.device)      #nx1\n",
    "\n",
    "        alpha=softmax(temp,edge_index[1])\n",
    "\n",
    "        rs=x_j*alpha                                                            #nxd\n",
    "        return rs\n",
    "\n",
    "\n",
    "sft = torch.nn.Softmax(dim=0)\n",
    "\n",
    "\n",
    "class HetGATConv(MessagePassing):\n",
    "    def __init__(self, in_hid, out_hid, negative_slope=0.2,norm=True,dual=True,global_weight=True):\n",
    "        super(HetGATConv, self).__init__(aggr='add')\n",
    "\n",
    "        self.in_hid = in_hid\n",
    "        self.out_hid = out_hid\n",
    "        self.negative_slope=negative_slope\n",
    "        self.norm=norm\n",
    "        self.dual=dual\n",
    "        self.global_weight=global_weight\n",
    "\n",
    "        \n",
    "        self.rel_wi=nn.Parameter(torch.Tensor(4,out_hid*2,1))\n",
    "        self.rel_bt=nn.Parameter(torch.Tensor(out_hid*2,1))\n",
    "        self.w_bt=nn.Linear(out_hid,out_hid,bias=False)\n",
    "        self.w_out=nn.Linear(out_hid,out_hid,bias=False)\n",
    "\n",
    "        self.out_norm=nn.LayerNorm(out_hid)\n",
    "\n",
    "        self.skip = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        glorot(self.rel_wi)\n",
    "        glorot(self.rel_bt)\n",
    "        \n",
    "\n",
    "    def forward(self, a_hid,p_hid, edge_idx, edge_type):\n",
    "\n",
    "        xi=p_hid[edge_idx[1]]\n",
    "        out_list=[]\n",
    "        num_edge_types=4\n",
    "        edg_list=[]\n",
    "        for i in range(num_edge_types):\n",
    "            mask = (edge_type == i)\n",
    "            edge_index = edge_idx[:, mask]\n",
    "            if mask.sum() !=0:\n",
    "                rs=self.w_bt(F.leaky_relu(self.propagate(edge_index, x=(a_hid,p_hid),edge_type=i),self.negative_slope))   #Nxd\n",
    "                out_list+=[rs]\n",
    "                edg_list+=[i]\n",
    "            \n",
    "        if self.dual:\n",
    "            rs_list=[]\n",
    "            for i in range(len(edg_list)):\n",
    "                conc=torch.cat((p_hid,out_list[i]),dim=1)                                       #Nx2d\n",
    "                rs=torch.matmul(conc,self.rel_bt)                                             #Nx1\n",
    "                rs_list+=[rs]\n",
    "\n",
    "            rs=torch.stack(rs_list)                                                         #Nxr\n",
    "            beta=F.softmax(rs,dim=0)                                                        #Nxr\n",
    "            res=0\n",
    "            for i in range(len(edg_list)):\n",
    "                res+=out_list[i]*beta[i]\n",
    "        else:\n",
    "            res=0\n",
    "            for i in range(len(edg_list)):\n",
    "                res+=out_list[i]\n",
    "        final_weight=torch.sigmoid(self.skip)\n",
    "        res = self.out_norm(F.gelu(res)* (final_weight) + p_hid* (1 - final_weight))\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def message(self,edge_index,x_i, x_j,edge_type):\n",
    "        \n",
    "        node_f = torch.cat((x_i, x_j), 1)                                       #nx2d\n",
    "\n",
    "        temp = torch.matmul(node_f, self.rel_wi[edge_type]).to(x_i.device)      #nx1\n",
    "\n",
    "        alpha=softmax(temp,edge_index[1])\n",
    "\n",
    "        rs=x_j*alpha                                                            #nxd\n",
    "        return rs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f4a8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def laplacian(kernel):\n",
    "    d1 = sum(kernel)\n",
    "    D_1 = torch.diag(d1)\n",
    "    #I_M = t.diag(t.ones(len(d1)))\n",
    "    L_D_1 = D_1 - kernel# + I_M\n",
    "    D_5 = D_1.rsqrt()\n",
    "    D_5 = torch.where(torch.isinf(D_5), torch.full_like(D_5, 0), D_5)\n",
    "    L_D_11 = torch.mm(D_5, L_D_1)\n",
    "    L_D_11 = torch.mm(L_D_11, D_5)\n",
    "    return L_D_11\n",
    "\n",
    "\n",
    "def normalized_embedding(embeddings):\n",
    "    #[row, col] = embeddings.size()\n",
    "    [row, col] = embeddings.shape\n",
    "    ne = torch.zeros([row, col])\n",
    "    for i in range(row):\n",
    "        if (max(embeddings[i, :]) - min(embeddings[i, :])) != 0:\n",
    "            ne[i, :] = (embeddings[i, :] - min(embeddings[i, :])) / (max(embeddings[i, :]) - min(embeddings[i, :]))\n",
    "        else:\n",
    "            ne[i, :] = (embeddings[i, :] - min(embeddings[i, :]))\n",
    "        #ne[i, :] = (embeddings[i, :] - t.mean(embeddings[i, :])) / (t.std(embeddings[i, :]))\n",
    "    return ne\n",
    "\n",
    "\n",
    "def getGipKernel(y, trans, gamma, normalized=False):\n",
    "    if trans:\n",
    "        y = y.T\n",
    "    if normalized:\n",
    "        y = normalized_embedding(y)\n",
    "    krnl = torch.mm(y, y.T)\n",
    "    krnl = krnl / torch.mean(torch.diag(krnl))\n",
    "    krnl = torch.exp(-kernelToDistance(krnl) * gamma)\n",
    "    #krnl = cosine_kernel(krnl, krnl)\n",
    "    return krnl\n",
    "\n",
    "\n",
    "def kernelToDistance(k):\n",
    "    di = torch.diag(k).T\n",
    "    d = di.repeat(len(k)).reshape(len(k), len(k)).T + di.repeat(len(k)).reshape(len(k), len(k)) - 2 * k\n",
    "    return d\n",
    "\n",
    "\n",
    "def cosine_kernel(tensor_1, tensor_2):\n",
    "    return torch.DoubleTensor([torch.cosine_similarity(tensor_1[i], tensor_2, dim=-1).tolist() for i in\n",
    "                           range(tensor_1.shape[0])])\n",
    "\n",
    "\n",
    "def normalized_kernel(K):\n",
    "    K = abs(K)\n",
    "    k = K.flatten().sort()[0]\n",
    "    min_v = k[torch.nonzero(k, as_tuple=False)[0]]\n",
    "    K[torch.where(K == 0)] = min_v\n",
    "    D = torch.diag(K)\n",
    "    D = D.sqrt()\n",
    "    S = K / (D * D.T)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "348d082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_het_mat(rna_dis_mat, dis_mat, rna_mat):\n",
    "    mat1 = np.hstack((rna_mat, rna_dis_mat))\n",
    "    mat2 = np.hstack((rna_dis_mat.T, dis_mat))\n",
    "    ret = np.vstack((mat1, mat2))\n",
    "    return ret\n",
    "\n",
    "def construct_adj_mat(training_mask):\n",
    "    adj_tmp = training_mask.copy()\n",
    "    rna_mat = np.zeros((training_mask.shape[0], training_mask.shape[0]))\n",
    "    dis_mat = np.zeros((training_mask.shape[1], training_mask.shape[1]))\n",
    "\n",
    "    mat1 = np.hstack((rna_mat, adj_tmp))\n",
    "    mat2 = np.hstack((adj_tmp.T, dis_mat))\n",
    "    ret = np.vstack((mat1, mat2))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feeb61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mylossw(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mylossw, self).__init__()\n",
    "        \n",
    "    def forward(self, target, prediction, miRNA_lap, dis_lap, alpha1, alpha2, phi1, phi2):\n",
    "        \n",
    "        loss_ls = torch.norm((target - prediction), p='fro') ** 2        \n",
    "        loss_ls = loss_ls.sum() \n",
    "        miRNA_reg = torch.trace(torch.mm(torch.mm(alpha1.T, miRNA_lap), alpha1))\n",
    "        dis_reg = torch.trace(torch.mm(torch.mm(alpha2.T, dis_lap), alpha2))\n",
    "        graph_reg = phi1 * miRNA_reg + phi2 * dis_reg\n",
    "\n",
    "        loss_sum = loss_ls + graph_reg\n",
    "\n",
    "        return loss_sum.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "658f9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRGATConv(nn.Module):\n",
    "    def __init__(self,in_hid_1, in_hid_2, out_hid,num_m1,num_m2,conv_name=\"hrgat\",n_heads=8,n_layers=2,\n",
    "                 dropout=0.2,norm=True,hgt_layer=2, feature_MFm = None, feature_MFd = None, sim_m = None, \n",
    "                 sim_d = None,  gamma = 1/128, phi1 = 1/256, phi2 = 1/256, **kwargs):\n",
    "        super(HRGATConv,self).__init__()\n",
    "        self.conv_name=conv_name\n",
    "        self.hetgat=nn.ModuleList()\n",
    "        self.layer=n_layers\n",
    "        self.hgt_layer = hgt_layer\n",
    "\n",
    "        self.feature_MFm = torch.Tensor(feature_MFm)\n",
    "        self.feature_MFd = torch.Tensor(feature_MFd)\n",
    "        self.sim_m, self.sim_d = torch.Tensor(sim_m), torch.Tensor(sim_d)\n",
    "        self.miRNA_size, self.dis_size = sim_m.shape[0], sim_d.shape[0]\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.phi1, self.phi2 = phi1, phi2\n",
    "\n",
    "        self.miRNA_l = []\n",
    "        self.dis_l = []\n",
    "\n",
    "        self.miRNA_k = []\n",
    "        self.dis_k = []\n",
    "        \n",
    "        self.alpha1 = torch.randn(self.miRNA_size, self.dis_size).double()\n",
    "        self.alpha2 = torch.randn(self.dis_size, self.miRNA_size).double()        \n",
    "\n",
    "        self.hgt=nn.ModuleList()\n",
    "        self.norm=nn.LayerNorm(out_hid)\n",
    "        self.drop=Dropout(dropout)\n",
    "        self.in_hid = 32\n",
    "        self.proj_a=gLinear(in_hid_2,out_hid,weight_initializer=\"kaiming_uniform\", bias=True)\n",
    "        self.proj_p=gLinear(in_hid_1,out_hid,weight_initializer=\"kaiming_uniform\", bias=True)\n",
    "        \n",
    "        self.CNN_drug = nn.Conv2d(in_channels = hgt_layer + n_layers,\n",
    "                       out_channels=128,\n",
    "                       kernel_size=(16, 1),\n",
    "                       stride=1,\n",
    "                       bias=True)\n",
    "        self.CNN_cir = nn.Conv2d(in_channels = hgt_layer + n_layers,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=(16, 1),\n",
    "                      stride=1,\n",
    "                      bias=True) \n",
    "        \n",
    "\n",
    "        for _ in range(hgt_layer):\n",
    "            if _ == 0:\n",
    "                if self.conv_name == \"rgcn\":\n",
    "                    self.hgt.append(RGCNConv(in_hid_1, out_hid, 4))\n",
    "                    self.hgt.append(RGCNConv(in_hid_2, out_hid, 4))\n",
    "                elif self.conv_name == \"dgat\":\n",
    "                    self.hgt.append(DGATConv(in_hid_1, out_hid, num_m1,heads=n_heads))\n",
    "                    self.hgt.append(DGATConv(in_hid_2, out_hid, num_m2,heads=n_heads))\n",
    "            else:\n",
    "                if self.conv_name == \"rgcn\":\n",
    "                    self.hgt.append(RGCNConv(out_hid, out_hid, 4))\n",
    "                    self.hgt.append(RGCNConv(out_hid, out_hid, 4))\n",
    "                elif self.conv_name == \"dgat\":\n",
    "                    self.hgt.append(DGATConv(out_hid, out_hid, num_m1,heads=n_heads))\n",
    "                    self.hgt.append(DGATConv(out_hid, out_hid, num_m2,heads=n_heads))\n",
    "\n",
    "        if self.conv_name == \"dhan1\":\n",
    "            for n in range(n_layers):\n",
    "                self.hetgat.append(HetGATConv(out_hid, out_hid,dual=False))\n",
    "                self.hetgat.append(HetGATConv(out_hid, out_hid,dual=False))\n",
    "        else:\n",
    "            for n in range(n_layers):\n",
    "                self.hetgat.append(HetGATConv(out_hid, out_hid))\n",
    "                self.hetgat.append(HetGATConv(out_hid, out_hid))\n",
    "    # _gh=(node_feature, edge_index, edge_type, id_list)\n",
    "        self.lenw = int(len(self.hgt)/2+len(self.hetgat)/2+2)\n",
    "        self.weight_mlp = (torch.ones(2, self.lenw)/self.lenw).double()\n",
    "        \n",
    "        self.params1 = list(self.hgt.parameters())\n",
    "        self.params2 = list(self.hetgat.parameters())\n",
    "        #self.params3 = list(self.proj_a.parameters())\n",
    "        #self.params4 = list(self.proj_p.parameters()) \n",
    "\n",
    "    def forward(self,m_f, d_f, edge_index_m,edge_index_d, edge_index_h, edge_type_m, edge_type_d, edge_type_h, device):\n",
    "\n",
    "        miRNA_kernels = []\n",
    "        dis_kernels = []\n",
    "        tem_m = []\n",
    "        tem_d = []\n",
    "        \n",
    "        miRNA_kernels.append(self.sim_m)\n",
    "        dis_kernels.append(self.sim_d)         \n",
    "                        \n",
    "        miRNA_kernels.append(torch.DoubleTensor(getGipKernel(self.feature_MFm, 0, self.gamma, True).double()))\n",
    "        dis_kernels.append(torch.DoubleTensor(getGipKernel(self.feature_MFd, 0, self.gamma, True).double()))\n",
    "\n",
    "        #h_p_cnn = self.proj_p(m_f.to(device))\n",
    "        #h_a_cnn = self.proj_a(d_f.to(device))\n",
    "\n",
    "        h_p=m_f.to(device)\n",
    "        h_a=d_f.to(device)\n",
    "        for hl in range(int((len(self.hgt)/2))):\n",
    "            if hl==0:\n",
    "                h_p=self.hgt[2*hl](h_p, edge_index_m.to(device), edge_type_m.to(device))\n",
    "                h_a=self.hgt[2*hl+1](h_a, edge_index_d.to(device), edge_type_d.to(device))\n",
    "                miRNA_kernels.append(torch.DoubleTensor(getGipKernel(h_p, 0, self.gamma, True).double()))\n",
    "                dis_kernels.append(torch.DoubleTensor(getGipKernel(h_a, 0, self.gamma, True).double()))\n",
    "                tem_m.append(h_p)\n",
    "                tem_d.append(h_a)\n",
    "            else:\n",
    "                h_p = self.hgt[2 * hl](h_p.to(device), edge_index_m.to(device), edge_type_m.to(device))\n",
    "                h_a = self.hgt[2 * hl + 1](h_a.to(device), edge_index_d.to(device), edge_type_d.to(device))\n",
    "                miRNA_kernels.append(torch.DoubleTensor(getGipKernel(h_p, 0, self.gamma, True).double()))\n",
    "                dis_kernels.append(torch.DoubleTensor(getGipKernel(h_a, 0, self.gamma, True).double()))\n",
    "                tem_m.append(h_p)\n",
    "                tem_d.append(h_a)\n",
    "\n",
    "        #h_p_cnn = self.proj_p(m_f.to(device))\n",
    "        #h_a_cnn = self.proj_a(d_f.to(device))\n",
    "        for ly in range(int(len(self.hetgat)/2)):\n",
    "            edge_index_h = torch.stack((edge_index_h[1],edge_index_h[0]))\n",
    "            p_hid = self.hetgat[2*ly](h_a, h_p, edge_index_h.to(device), edge_type_h.to(device))\n",
    "            miRNA_kernels.append(torch.DoubleTensor(getGipKernel(p_hid, 0, self.gamma, True).double()))\n",
    "\n",
    "            edge_index_h = torch.stack((edge_index_h[1],edge_index_h[0]))\n",
    "            a_hid = self.hetgat[2*ly+1](h_p, h_a, edge_index_h.to(device), edge_type_h.to(device))\n",
    "            dis_kernels.append(torch.DoubleTensor(getGipKernel(a_hid, 0, self.gamma, True).double()))\n",
    "            tem_m.append(p_hid)\n",
    "            tem_d.append(a_hid)\n",
    "\n",
    "            #edge_indx_h = torch.stack((edge_index_h[1], edge_index_h[0]))\n",
    "            h_a=a_hid\n",
    "            h_p=p_hid\n",
    "        #h_a=self.drop(self.norm(h_a))\n",
    "        #h_p=self.drop(self.norm(h_p))\n",
    "        #A_pre = h_p@h_a.T\n",
    "        #cnn_embd_cir = h_p_cnn\n",
    "        \"\"\"\n",
    "        cnn_embd_cir = tem_m[0]\n",
    "        for i in range(1, len(tem_m)):            \n",
    "            cnn_embd_cir = torch.cat((cnn_embd_cir, tem_m[i]), 1)\n",
    "        cnn_embd_cir = cnn_embd_cir.t().view(1, len(tem_m), 16, self.miRNA_size)\n",
    "        cnn_embd_cir = self.CNN_cir(cnn_embd_cir)\n",
    "        cnn_embd_cir = cnn_embd_cir.view(128, self.miRNA_size).t()\n",
    "        h_p = cnn_embd_cir\n",
    "        \n",
    "        cnn_embd_drug = tem_d[0]       \n",
    "        for i in range(1, len(tem_d)):            \n",
    "            cnn_embd_drug = torch.cat((cnn_embd_drug, tem_d[i]), 1)\n",
    "        cnn_embd_drug = cnn_embd_drug.t().view(1, len(tem_d), 16, self.dis_size)\n",
    "        cnn_embd_drug = self.CNN_drug(cnn_embd_drug)\n",
    "        cnn_embd_drug = cnn_embd_drug.view(128, self.dis_size).t()\n",
    "        h_a = cnn_embd_drug\n",
    "        \"\"\"\n",
    "        #miRNA_kernels.append(torch.DoubleTensor(getGipKernel(h_p, 0, self.gamma, True).double()))\n",
    "        #dis_kernels.append(torch.DoubleTensor(getGipKernel(h_a, 0, self.gamma, True).double()))\n",
    "        \n",
    "        miRNA_k = sum([(1/len(miRNA_kernels)) * miRNA_kernels[i] for i in range(len(miRNA_kernels))])\n",
    "        self.miRNA_k = normalized_kernel(miRNA_k)\n",
    "        dis_k = sum([(1/len(dis_kernels)) * dis_kernels[i] for i in range(len(dis_kernels))])        \n",
    "        self.dis_k = normalized_kernel(dis_k)\n",
    "        \n",
    "        self.miRNA_l = laplacian(miRNA_k)\n",
    "        self.dis_l = laplacian(dis_k)\n",
    "\n",
    "        out1 = torch.mm(self.miRNA_k, self.alpha1)\n",
    "        out2 = torch.mm(self.dis_k, self.alpha2)\n",
    "\n",
    "        #out = (out1 + out2.T + h_p@h_a.T) / 3 \n",
    "        out = (out1 + out2.T) / 2\n",
    "\n",
    "        #return A_pre\n",
    "        return out, miRNA_kernels, dis_kernels\n",
    "        #return h_p@h_a.T, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6926847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_matrix(matrix, k=20):\n",
    "    num = matrix.shape[0]\n",
    "    knn_graph = np.zeros(matrix.shape)\n",
    "    idx_sort = np.argsort(-(matrix - np.eye(num)), axis=1)\n",
    "    for i in range(num):\n",
    "        #将第i行最大的前k个值赋值给knn_graph(确保是对称矩阵)\n",
    "        knn_graph[i, idx_sort[i, :k + 1]] = matrix[i, idx_sort[i, :k + 1]]\n",
    "        knn_graph[idx_sort[i, :k + 1], i] = matrix[idx_sort[i, :k + 1], i]\n",
    "    return knn_graph + np.eye(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4b952a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validation_experiment_3(edge_idx_dict, A, k_fold = 5, k1 = 27, k2 = 22, seq_sim_matrix = None, str_sim_matrix = None,\n",
    "                  lr = 0.05, weight_decay = 0.0005, threshold_m = 15, threshold_d = 19,\n",
    "                  dropout = 0.1, device = device, hgt_layer = 1, k = 25, epoch = 50,\n",
    "                  n_layers = 1, n_heads = 1, conv_name = 'dgat', num_m1 = 4, num_m2 = 4, out_hid = 16,\n",
    "                  gamma = 1/128, phi1 = 1/100, phi2 = 1/100):\n",
    "    #A = miRNA_dis_matrix\n",
    "        \n",
    "    metric = np.zeros((1, 7))\n",
    "    pre_matrix = np.zeros(A.shape)\n",
    "    \n",
    "    pos_edges = edge_idx_dict['pos_edges']\n",
    "    neg_edges = edge_idx_dict['neg_edges']\n",
    "    idx = np.arange(pos_edges.shape[1])\n",
    "    np.random.shuffle(idx)\n",
    "    idx_splited = np.array_split(idx, k_fold)        \n",
    "       \n",
    "    \n",
    "    for i in range(k_fold):\n",
    "        print(\"------this is %dth cross validation------\" % (i + 1))\n",
    "        tmp = []\n",
    "        for j in range(1, k_fold):\n",
    "            tmp.append(idx_splited[(j + i) % k_fold])\n",
    "        tmp = np.concatenate(tmp)\n",
    "        training_pos_edges = pos_edges[:, tmp]\n",
    "        print(\"training_pos_edges.shape: {}\".format(training_pos_edges.shape))\n",
    "        training_neg_edges = neg_edges[:, tmp]\n",
    "        test_pos_edges = pos_edges[:, idx_splited[i]]\n",
    "        test_neg_edges = neg_edges[:, idx_splited[i]]\n",
    "        temp_drug_dis = np.zeros((A.shape[0], A.shape[1]))\n",
    "        temp_drug_dis[training_pos_edges[0], training_pos_edges[1]] = 1        \n",
    "        new_A = temp_drug_dis\n",
    "        sim_d, sim_m = skf(new_A, str_sim_matrix, seq_sim_matrix, k2, k1)\n",
    "        sim_m_0 = set_digo_zero(sim_m, 0)\n",
    "        sim_d_0 = set_digo_zero(sim_d, 0)        \n",
    "        \n",
    "        m_adj = k_matrix(sim_m, k = k)\n",
    "        d_adj = k_matrix(sim_d, k = k)\n",
    "        m_adj_0 = set_digo_zero(m_adj, 1)\n",
    "        d_adj_0 = set_digo_zero(d_adj, 1)\n",
    "        \n",
    "        #trainlu = np.hstack((training_neg_edges, training_pos_edges))\n",
    "        #testlu = np.hstack((test_neg_edges, test_pos_edges))\n",
    "        #trainlu = np.vstack((trainlu[1], trainlu[0]))\n",
    "        #testlu = np.vstack((testlu[1], testlu[0]))\n",
    "                \n",
    "\n",
    "        feature_MFd1, feature_MFm1 = d_adj@new_A, m_adj@new_A.T\n",
    "        #feature_MFd1, feature_MFm1 = torch.Tensor(d_adj)@feature_d_Z.detach(), torch.Tensor(m_adj)@feature_m_Z.detach()\n",
    "        feature_MFm1, feature_MFd1 = torch.Tensor(feature_MFm1),torch.Tensor(feature_MFd1)\n",
    "        #feature_MFd1, feature_MFm1 = feature_d_Z.detach(), feature_m_Z.detach()\n",
    "        \n",
    "        gnn = None\n",
    "        \n",
    "        gnn = HRGATConv(in_hid_1 = feature_MFm1.shape[1], in_hid_2 = feature_MFd1.shape[1], out_hid = out_hid, num_m1 = num_m1, \n",
    "                        num_m2 = num_m2, conv_name = conv_name, n_heads= n_heads, n_layers = n_layers, \n",
    "                        dropout = dropout, hgt_layer = hgt_layer, feature_MFm = feature_MFm1, feature_MFd = feature_MFd1, \n",
    "                        sim_m = sim_m, sim_d = sim_d, gamma = gamma, phi1 = phi1, phi2 = phi2).to(device)\n",
    "\n",
    "\n",
    "        \n",
    "        optimizer = optim.Adam([{'params':gnn.params1, 'weight_decay':0.001},{'params':gnn.params2, 'weight_decay':0.001},], lr = lr)\n",
    "        #optimizer = optim.Adam(gnn.parameters(), weight_decay = weight_decay, lr = lr)\n",
    "        \n",
    "        \n",
    "        regression_critgnn = Mylossw()\n",
    "        #regression_critMLP = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "\n",
    "        edge_index_m, edge_type_m = get_edge_index(m_adj, new_A.T, threshold = threshold_m)\n",
    "        edge_index_d, edge_type_d = get_edge_index(d_adj, new_A, threshold = threshold_d)\n",
    "        \n",
    "        edge_index_h, edge_type_h = get_edge_index_h(new_A.T, threshold_m = threshold_m, threshold_d = threshold_d)\n",
    "\n",
    "        new_A = new_A.T\n",
    "        for i in range(epoch):\n",
    "            gnn.zero_grad()    \n",
    "            \n",
    "            PRE, m_k, d_k = gnn(feature_MFm1, feature_MFd1, edge_index_m,edge_index_d, edge_index_h, \n",
    "                  edge_type_m, edge_type_d, edge_type_h, device)      \n",
    "            \n",
    "            #\"\"\"\n",
    "            gnn.alpha1 = torch.mm(\n",
    "            torch.mm((torch.mm(gnn.miRNA_k, gnn.miRNA_k) + gnn.phi1 * gnn.miRNA_l).inverse(), \n",
    "                 gnn.miRNA_k),2 * torch.Tensor(new_A) - torch.mm(gnn.alpha2.T, gnn.dis_k.T)).detach()\n",
    "            gnn.alpha2 = torch.mm(torch.mm((torch.mm(gnn.dis_k, gnn.dis_k) + gnn.phi2 * gnn.dis_l).inverse(), gnn.dis_k),\n",
    "                  2 * torch.Tensor(new_A).T - torch.mm(gnn.alpha1.T, gnn.miRNA_k.T)).detach()\n",
    "    \n",
    "    \n",
    "    \n",
    "            loss = regression_critgnn(torch.Tensor(new_A), PRE, gnn.miRNA_l, gnn.dis_l, gnn.alpha1,\n",
    "                           gnn.alpha2, gnn.phi1, gnn.phi2)\n",
    "            #\"\"\"\n",
    "            #loss = regression_critMLP(PRE[tuple(np.array(trainlu))], torch.Tensor(new_A[tuple(np.array(trainlu))]))\n",
    "                    \n",
    "            loss = loss.requires_grad_()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            if i%100 == 0:\n",
    "                print(loss)\n",
    "\n",
    "\n",
    "        new_A = new_A.T\n",
    "        gnn.eval()\n",
    "        \n",
    "        PRE_test, m_k_t, d_k_t = gnn(feature_MFm1, feature_MFd1, edge_index_m,edge_index_d, edge_index_h, \n",
    "                  edge_type_m, edge_type_d, edge_type_h, device) \n",
    "\n",
    "        #INN0 = PRE_test.detach().numpy()\n",
    "        metric_tmp = calculate_evaluation_metrics(PRE_test.detach().T, test_pos_edges, test_neg_edges)\n",
    "\n",
    "        print(metric_tmp)\n",
    "        metric = metric + metric_tmp\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"{} coval\".format(k_fold))\n",
    "    print(\"auc[0, 0], aupr[0, 0], f1_score, accuracy, recall, specificity, precision\")\n",
    "    print(metric / k_fold)\n",
    "    metric = np.array(metric / k_fold)\n",
    "    return metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e9316ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 27\n",
    "k2 = 22\n",
    "drug_sim, cir_sim, edge_idx_dict, drug_cir_matrix = load_data()\n",
    "A = drug_cir_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3fbdf68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------this is 1th cross validation------\n",
      "training_pos_edges.shape: (2, 3307)\n",
      "tensor(6945137.8885, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      " auc:0.9309 ,aupr:0.9362,f1_score:0.8618, accuracy:0.8561, recall:0.8972, specificity:0.8150, precision:0.8291\n",
      "[0.9308700172093883, 0.9361883821468933, 0.8617886178861789, 0.8561064087061668, 0.8972188633615478, 0.814993954050786, 0.829050279329609]\n",
      "------this is 2th cross validation------\n",
      "training_pos_edges.shape: (2, 3307)\n",
      "tensor(7048559.8945, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      " auc:0.9014 ,aupr:0.9135,f1_score:0.8330, accuracy:0.8349, recall:0.8235, specificity:0.8464, precision:0.8428\n",
      "[0.9013501401461261, 0.9134607314176039, 0.8330275229357799, 0.8349455864570737, 0.8234582829504232, 0.8464328899637243, 0.8428217821782178]\n",
      "------this is 3th cross validation------\n",
      "training_pos_edges.shape: (2, 3307)\n",
      "tensor(7368466.3480, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      " auc:0.9147 ,aupr:0.9219,f1_score:0.8453, accuracy:0.8416, recall:0.8658, specificity:0.8174, precision:0.8258\n",
      "[0.9146643876776681, 0.921865954063214, 0.8453364817001181, 0.841596130592503, 0.8657799274486094, 0.8174123337363967, 0.825836216839677]\n",
      "------this is 4th cross validation------\n",
      "training_pos_edges.shape: (2, 3307)\n",
      "tensor(7171303.5510, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      " auc:0.9200 ,aupr:0.9269,f1_score:0.8497, accuracy:0.8470, recall:0.8646, specificity:0.8295, precision:0.8353\n",
      "[0.9199880396941786, 0.9269307582237837, 0.8496732026143791, 0.847037484885127, 0.8645707376058042, 0.8295042321644498, 0.8352803738317757]\n",
      "------this is 5th cross validation------\n",
      "training_pos_edges.shape: (2, 3308)\n",
      "tensor(7066842.0106, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      " auc:0.9191 ,aupr:0.9278,f1_score:0.8487, accuracy:0.8444, recall:0.8729, specificity:0.8160, precision:0.8259\n",
      "[0.9191397616213968, 0.9277583539992422, 0.8487345497351383, 0.8444309927360775, 0.8728813559322034, 0.8159806295399515, 0.8258877434135166]\n",
      "5 coval\n",
      "auc[0, 0], aupr[0, 0], f1_score, accuracy, recall, specificity, precision\n",
      "[[0.91720247 0.92524084 0.84771207 0.84482332 0.86478183 0.82486481\n",
      "  0.83177528]]\n"
     ]
    }
   ],
   "source": [
    "result = cross_validation_experiment_3(edge_idx_dict, A, k_fold = 5, k1 = 25, k2 = 25, seq_sim_matrix = cir_sim, \n",
    "                  str_sim_matrix = drug_sim,\n",
    "                  lr = 0.05, weight_decay = 0.01, threshold_m = 18, threshold_d = 37,\n",
    "                  dropout = 0.026, device = device, hgt_layer = 1, k = 25, epoch = 40,\n",
    "                  n_layers = 1, n_heads = 5, conv_name = 'dgat', num_m1 = 4, num_m2 = 4, out_hid = 16,\n",
    "                  gamma = 1/75, phi1 = 1/120, phi2 = 1/120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lr = 0.05, weight_decay = 0.01, threshold_m = 15, threshold_d = 19,dropout = 0.02, device = device, hgt_layer = 1, \n",
    "k = 25, epoch = 50,n_layers = 1, n_heads = 1, conv_name = 'dgat', num_m1 = 4, num_m2 = 4, out_hid = 16,\n",
    "gamma = 1/128, phi1 = 1/100, phi2 = 1/100\n",
    "[[0.91079151 0.91802955 0.84281533 0.84300558 0.84083373 0.84517744  0.84550056]]\n",
    "\n",
    "lr = 0.05, weight_decay = 0.01, threshold_m = 15, threshold_d = 19,dropout = 0.02, device = device, hgt_layer = 1, \n",
    "k = 25, epoch = 50,n_layers = 1, n_heads = 1, conv_name = 'dgat', num_m1 = 4, num_m2 = 4, out_hid = 16,\n",
    "gamma = 1/128, phi1 = 1/100, phi2 = 1/100  set_digo_zero(d_adj, 1)\n",
    "[[0.90982799 0.91853294 0.84172715 0.84167972 0.84155338 0.84180606  0.84230898]]\n",
    "\n",
    "lr = 0.05, weight_decay = 0.01, threshold_m = 15, threshold_d = 19,dropout = 0.02, device = device, hgt_layer = 1, \n",
    "k = 25, epoch = 50,n_layers = 1, n_heads = 1, conv_name = 'dgat', num_m1 = 4, num_m2 = 4, out_hid = 16,\n",
    "gamma = 1/128, phi1 = 1/100, phi2 = 1/100  set_digo_zero(d_adj, 1)\n",
    "[[0.91104823 0.91897384 0.84544202 0.84313045 0.85824225 0.82801866  0.83341387]]\n",
    "\n",
    "k1 = 25, k2 = 27, lr = 0.05, weight_decay = 0.01, threshold_m = 25, threshold_d = 36, dropout = 0.02, hgt_layer = 1, \n",
    "k = 25, epoch = 60, n_layers = 1, n_heads = 5, conv_name = 'dgat', num_m1 = 4, num_m2 = 4, out_hid = 16,\n",
    "gamma = 1/75, phi1 = 1/100, phi2 = 1/100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
